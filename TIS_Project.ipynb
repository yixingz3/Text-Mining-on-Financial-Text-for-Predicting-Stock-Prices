{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_pretrained_bert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vu3qQZVmXNsj",
        "outputId": "675c7c4a-90c4-4128-debb-40d757e1cc68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch_pretrained_bert\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 6.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (1.21.6)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (1.13.0+cu116)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.26.26-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 56.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.4.0)\n",
            "Collecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting botocore<1.30.0,>=1.29.26\n",
            "  Downloading botocore-1.29.26-py3-none-any.whl (10.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.2 MB 71.3 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.8/dist-packages (from botocore<1.30.0,>=1.29.26->boto3->pytorch_pretrained_bert) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.26->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 10.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->pytorch_pretrained_bert) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed boto3-1.26.26 botocore-1.29.26 jmespath-1.0.1 pytorch-pretrained-bert-0.6.2 s3transfer-0.6.0 urllib3-1.25.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report,roc_auc_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data import TensorDataset,RandomSampler,SequentialSampler\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch import optim\n",
        "import time  \n",
        "from pytorch_pretrained_bert import BertModel, BertTokenizer, BertConfig, BertAdam\n",
        "from tqdm import tqdm "
      ],
      "metadata": {
        "id": "GBzkrSVRsoft"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "is3ZVU3plzQT",
        "outputId": "f09dd2a9-63e4-4c0f-c781-522650a7ea41"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get the Data"
      ],
      "metadata": {
        "id": "fHOiDc9ULg1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/CS410Project/410_dataset.csv', sep=',')\n",
        "data = data[data.datetime > '2019-01-01']\n",
        "data = data.dropna()\n",
        "data = data[(data.label == 1) | (data.label == -1)]\n",
        "data.label = data.label.apply(lambda x:0 if x==-1 else 1)"
      ],
      "metadata": {
        "id": "odwEi9-buHi8"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PU-B0V7YlRRB",
        "outputId": "e83dc4c3-542d-4e5f-b117-e3a94bccec9c"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(611764, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "3ZQ7C8881kUi",
        "outputId": "5a38528a-08fa-41d5-82f7-b2bd7407bf51"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  symbol                                            message  \\\n",
              "0   AAPL  peak profit last 6 expired option alerts aapl ...   \n",
              "1   AAPL  aapl jul 17 382 50 calls option volume 144 44 ...   \n",
              "2   AAPL  tsla market true bubble territory profitable c...   \n",
              "3   AAPL  aapl analyzed 26 analysts buy consensus 86 ana...   \n",
              "4   AAPL    aapl new article dogs dow august 4 adopt ignore   \n",
              "\n",
              "              datetime     user  message_id        Date      Time  label  \n",
              "0  2020-07-19 09:49:35  1442893   229008387  2020-07-19  09:49:35      1  \n",
              "1  2020-07-19 09:47:26  1442893   229008357  2020-07-19  09:47:26      1  \n",
              "2  2020-07-19 09:01:25  1115913   229007569  2020-07-19  09:01:25      1  \n",
              "3  2020-07-19 08:13:00    47688   229006733  2020-07-19  08:13:00      1  \n",
              "4  2020-07-19 07:54:05  1555408   229006403  2020-07-19  07:54:05      1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-49e8b20d-ce51-48b6-9e3d-7660d54d70f8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>symbol</th>\n",
              "      <th>message</th>\n",
              "      <th>datetime</th>\n",
              "      <th>user</th>\n",
              "      <th>message_id</th>\n",
              "      <th>Date</th>\n",
              "      <th>Time</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AAPL</td>\n",
              "      <td>peak profit last 6 expired option alerts aapl ...</td>\n",
              "      <td>2020-07-19 09:49:35</td>\n",
              "      <td>1442893</td>\n",
              "      <td>229008387</td>\n",
              "      <td>2020-07-19</td>\n",
              "      <td>09:49:35</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AAPL</td>\n",
              "      <td>aapl jul 17 382 50 calls option volume 144 44 ...</td>\n",
              "      <td>2020-07-19 09:47:26</td>\n",
              "      <td>1442893</td>\n",
              "      <td>229008357</td>\n",
              "      <td>2020-07-19</td>\n",
              "      <td>09:47:26</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AAPL</td>\n",
              "      <td>tsla market true bubble territory profitable c...</td>\n",
              "      <td>2020-07-19 09:01:25</td>\n",
              "      <td>1115913</td>\n",
              "      <td>229007569</td>\n",
              "      <td>2020-07-19</td>\n",
              "      <td>09:01:25</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AAPL</td>\n",
              "      <td>aapl analyzed 26 analysts buy consensus 86 ana...</td>\n",
              "      <td>2020-07-19 08:13:00</td>\n",
              "      <td>47688</td>\n",
              "      <td>229006733</td>\n",
              "      <td>2020-07-19</td>\n",
              "      <td>08:13:00</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AAPL</td>\n",
              "      <td>aapl new article dogs dow august 4 adopt ignore</td>\n",
              "      <td>2020-07-19 07:54:05</td>\n",
              "      <td>1555408</td>\n",
              "      <td>229006403</td>\n",
              "      <td>2020-07-19</td>\n",
              "      <td>07:54:05</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-49e8b20d-ce51-48b6-9e3d-7660d54d70f8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-49e8b20d-ce51-48b6-9e3d-7660d54d70f8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-49e8b20d-ce51-48b6-9e3d-7660d54d70f8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.groupby(\"symbol\").message.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAodC4XOLyso",
        "outputId": "fb3d6834-469a-4b96-c0d5-2f391faaf3c2"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "symbol\n",
              "AAPL     262204\n",
              "AMZN     127202\n",
              "FB        45864\n",
              "GOOGL     23205\n",
              "NFLX     153289\n",
              "Name: message, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectorize + Machine Learning Model"
      ],
      "metadata": {
        "id": "IsYJiXpCMAo9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the data\n",
        "\n",
        "X = data.message\n",
        "y = data.label\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=2022)"
      ],
      "metadata": {
        "id": "-3Mwk1_uT74S"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the performance\n",
        "\n",
        "def evaluate_performance(y_pred,y_test):\n",
        "  accuracy = np.mean(y_pred==y_test)\n",
        "  print(f\"The accuarcy is {accuracy}\")\n",
        "  print(classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "id": "8G3C4ituMG1g"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model part\n",
        "\n",
        "def vectorize_machine_learning(X_train, X_test, y_train, y_test,vectorize,ml):\n",
        "\n",
        "  '''\n",
        "  For vectorize model, 1 represents CountVectorizer, 2 represents TfidfVectorizer()\n",
        "  For machine learning model, 1 represents logistic regression, 2 represents Naive Bayes, 3 represents Random Forest\n",
        "  '''\n",
        "\n",
        "  vectorize_model_set = {1:CountVectorizer(),2:TfidfVectorizer()}\n",
        "  ml_model_set = {1:LogisticRegression(penalty=\"l1\",solver=\"liblinear\",C=10000),2:MultinomialNB(),\n",
        "                  3:RandomForestClassifier(max_depth=5,n_estimators=200,class_weight='balanced')}\n",
        "  vectorize = vectorize_model_set[vectorize]\n",
        "  model = ml_model_set[ml]\n",
        "  train_features = vectorize.fit_transform(X_train)\n",
        "  test_features = vectorize.transform(X_test)\n",
        "  model.fit(train_features,y_train)\n",
        "  y_pred = model.predict(test_features)\n",
        "\n",
        "  evaluate_performance(y_pred,y_test)"
      ],
      "metadata": {
        "id": "t8jBhyYr4fgP"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CountVectorizer + LogisticRegression\n",
        "\n",
        "vectorize_machine_learning(X_train, X_test, y_train, y_test,1,1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6VcNDXR68Rm",
        "outputId": "0173c96b-b313-423d-e5de-375649236029"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuarcy is 0.5912352681563333\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.34      0.42     26358\n",
            "           1       0.61      0.78      0.69     34819\n",
            "\n",
            "    accuracy                           0.59     61177\n",
            "   macro avg       0.58      0.56      0.55     61177\n",
            "weighted avg       0.58      0.59      0.57     61177\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TfidfVectorizer + Naive Bayes\n",
        "\n",
        "vectorize_machine_learning(X_train, X_test, y_train, y_test,2,2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3S-3Ni873Tue",
        "outputId": "31c31a0d-7cc7-41c2-e297-0f663beae51f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuarcy is 0.5885545221243278\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.16      0.25     26358\n",
            "           1       0.59      0.91      0.72     34819\n",
            "\n",
            "    accuracy                           0.59     61177\n",
            "   macro avg       0.59      0.54      0.48     61177\n",
            "weighted avg       0.59      0.59      0.51     61177\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CountVectorizer + RandomForest\n",
        "\n",
        "vectorize_machine_learning(X_train, X_test, y_train, y_test,1,3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AixW2TnDB_G",
        "outputId": "d84e880c-850b-43db-b3ee-6765074655a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuarcy is 0.5712277489906338\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.40      0.44     26358\n",
            "           1       0.61      0.70      0.65     34819\n",
            "\n",
            "    accuracy                           0.57     61177\n",
            "   macro avg       0.55      0.55      0.55     61177\n",
            "weighted avg       0.56      0.57      0.56     61177\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning Model - LSTM"
      ],
      "metadata": {
        "id": "voPmCHP1NnYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the data\n",
        "\n",
        "train_data_lstm = pd.DataFrame(X_train)\n",
        "train_data_lstm[\"label\"] = y_train\n",
        "test_data_lstm = pd.DataFrame(X_test)\n",
        "test_data_lstm[\"label\"] = y_test"
      ],
      "metadata": {
        "id": "262yW2KuNy7B"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)"
      ],
      "metadata": {
        "id": "6WFU8ZymB7t3"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mydict = Dictionary()\n",
        "mydict.add_word(\"UNK\")\n",
        "for item in train_data_lstm.iterrows():\n",
        "    line = item[1][\"message\"] \n",
        "    words = line.split(\" \") \n",
        "    for word in words:\n",
        "        word = word.strip()\n",
        "        if word:\n",
        "            mydict.add_word(word)"
      ],
      "metadata": {
        "id": "mc6XbCPMB80o"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Corpus(object):\n",
        "    def __init__(self,length,data,corpus_dict):\n",
        "        self.dictionary = corpus_dict\n",
        "        self.data = data\n",
        "        self.texts,self.labels = self.tokenize(length)\n",
        "\n",
        "    def tokenize(self,length):\n",
        "        token_text = []\n",
        "        labels = []\n",
        "        for item in self.data.iterrows():\n",
        "            line = item[1][\"message\"] \n",
        "            labels.append(int(item[1][\"label\"])) \n",
        "            words = line.split(\" \") \n",
        "            text = torch.LongTensor(np.zeros(length, dtype=np.int64))\n",
        "            for index,word in enumerate(words[:length]):\n",
        "                word = word.strip()\n",
        "                if word:\n",
        "                  if word in self.dictionary.word2idx:\n",
        "                    text[index] = self.dictionary.word2idx[word]\n",
        "                  else:\n",
        "                    text[index] = self.dictionary.word2idx[\"UNK\"]\n",
        "            token_text.append(text)\n",
        "        return token_text,labels"
      ],
      "metadata": {
        "id": "cKVsbHKKjZgK"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self,length, corpus):\n",
        "        corpus = corpus\n",
        "        self.token_text = corpus.texts\n",
        "        self.labels = corpus.labels\n",
        "        self.length = length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = self.token_text[index]\n",
        "        label = torch.LongTensor([self.labels[index]])\n",
        "        return text, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ],
      "metadata": {
        "id": "D5M1h7h7lpYU"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# construct dataset\n",
        "\n",
        "train_corpus = Corpus(100,train_data_lstm,mydict)\n",
        "train_set = MyDataset(100, train_corpus)\n",
        "train_loader = DataLoader(train_set,\n",
        "                          batch_size=64,\n",
        "                          shuffle=True)\n",
        "\n",
        "test_corpus = Corpus(100,test_data_lstm,mydict)\n",
        "test_set = MyDataset(100, test_corpus)\n",
        "test_loader = DataLoader(test_set,\n",
        "                          batch_size=64,\n",
        "                          shuffle=True)"
      ],
      "metadata": {
        "id": "xM62xSOJlsz3"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model part\n",
        "\n",
        "class MyClassifier(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, label_size, batch_size,device):\n",
        "        super(MyClassifier, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim,device=self.device)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,num_layers=1,device=self.device)\n",
        "        self.fc = nn.Linear(hidden_dim, label_size,device=self.device)\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        h0 = Variable(torch.zeros(1, self.batch_size, self.hidden_dim).to(self.device))\n",
        "        c0 = Variable(torch.zeros(1, self.batch_size, self.hidden_dim).to(self.device))\n",
        "        return (h0, c0)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embedding = self.embedding(sentence.to(self.device))\n",
        "        embedding = embedding.to(self.device)\n",
        "        x = embedding.view(len(sentence), self.batch_size, -1)\n",
        "        output, _ = self.lstm(x.to(self.device), self.hidden)\n",
        "        result = self.fc(output[-1])\n",
        "        return result\n"
      ],
      "metadata": {
        "id": "DMs8uxr7pzcc"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "hidden_dim = 64\n",
        "vocab_size = len(train_corpus.dictionary)\n",
        "label_size = 2\n",
        "batch_size = 64\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "LSTM_model = MyClassifier(embedding_dim, hidden_dim, vocab_size, label_size, batch_size, device)"
      ],
      "metadata": {
        "id": "yxslwMNVrIp9"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(LSTM_model.parameters(), lr=0.01)\n",
        "loss_function = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "c3JYRJldq4aj"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(LSTM_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZY4VaSrssMr",
        "outputId": "faf349e7-ec0f-4a08-e78d-b6382c2649d7"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyClassifier(\n",
            "  (embedding): Embedding(79574, 100)\n",
            "  (lstm): LSTM(100, 64)\n",
            "  (fc): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train and test - need to run again to train model with a different dataset\n",
        "\n",
        "train_loss = []\n",
        "train_acc = []\n",
        "test_loss = []\n",
        "test_acc = []\n",
        "for epoch in range(10):\n",
        "    total_acc = 0\n",
        "    total_loss = 0\n",
        "    total = 0\n",
        "    for iter, traindata in enumerate(train_loader):\n",
        "        train_inputs, train_labels = traindata\n",
        "        train_labels = torch.squeeze(train_labels)\n",
        "        if train_labels.shape[0] !=64:\n",
        "          continue\n",
        "        train_inputs = train_inputs.to(device)\n",
        "        train_labels = train_labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = LSTM_model(train_inputs.t())\n",
        "        loss = loss_function(output, Variable(train_labels))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        total_acc = total_acc + (predicted == train_labels).sum()\n",
        "        total_loss = total_loss+loss.item()\n",
        "        total = total + len(train_labels)\n",
        "    train_loss.append(total_loss / total)\n",
        "    train_acc.append(total_acc / total)\n",
        "\n",
        "    total_acc = 0\n",
        "    total_loss = 0\n",
        "    total = 0\n",
        "    for iter, testdata in enumerate(test_loader):\n",
        "        test_inputs, test_labels = testdata\n",
        "        test_labels = torch.squeeze(test_labels)\n",
        "        if test_labels.shape[0] !=64:\n",
        "          continue\n",
        "        test_inputs = test_inputs.to(device)\n",
        "        test_labels = test_labels.to(device)\n",
        "\n",
        "        output = LSTM_model(test_inputs.t())\n",
        "        loss = loss_function(output, Variable(test_labels))\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        total_acc = total_acc + (predicted == test_labels).sum()\n",
        "        total_loss = total_loss+loss.item()\n",
        "        total = total + len(test_labels)\n",
        "    test_loss.append(total_loss / total)\n",
        "    test_acc.append(total_acc / total)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}:train loss:{train_loss[epoch]},train acc:{train_acc[epoch]},test loss:{test_loss[epoch]},test acc:{test_acc[epoch]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5IziDUyswa6",
        "outputId": "1144b0fe-f8fe-4588-bd35-2fef572e9a83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1:train loss:0.010682805093454395,train acc:0.5712025761604309,test loss:0.010705553671998503,test acc:0.569126307964325\n",
            "Epoch 2:train loss:0.010689118815123883,train acc:0.5712298154830933,test loss:0.010710821121306944,test acc:0.5688972473144531\n",
            "Epoch 3:train loss:0.010687578676307001,train acc:0.5712952017784119,test loss:0.010697662590451889,test acc:0.5688154697418213\n",
            "Epoch 4:train loss:0.010686696929390716,train acc:0.5712624788284302,test loss:0.010689954758080513,test acc:0.5690935850143433\n",
            "Epoch 5:train loss:0.010687253583340424,train acc:0.5710354447364807,test loss:0.010693868696299524,test acc:0.5691426992416382\n",
            "Epoch 6:train loss:0.010680538808424143,train acc:0.5708792209625244,test loss:0.010692382478082055,test acc:0.5688809156417847\n",
            "Epoch 7:train loss:0.010671421227977979,train acc:0.5703688263893127,test loss:0.01069180381664743,test acc:0.5691754221916199\n",
            "Epoch 8:train loss:0.01066876040105538,train acc:0.5695078372955322,test loss:0.010699975919661098,test acc:0.5689626932144165\n",
            "Epoch 9:train loss:0.010654924944748201,train acc:0.5707193613052368,test loss:0.01074756537881041,test acc:0.5691426992416382\n",
            "Epoch 10:train loss:0.010670450812375004,train acc:0.5709863901138306,test loss:0.010701172391235516,test acc:0.5690445303916931\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save model - need to run again if model is trained with a different dataset\n",
        "\n",
        "PATH = \"lstm_model.pth\"\n",
        "torch.save(LSTM_model.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "GCvRTIvGpFY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load model - run with current dataset\n",
        "\n",
        "LSTM_model = MyClassifier(embedding_dim, hidden_dim, vocab_size, label_size, batch_size, device)\n",
        "LSTM_model.load_state_dict(torch.load(\"/content/drive/MyDrive/CS410Project/lstm_model.pth\"))\n",
        "\n",
        "total_acc = 0\n",
        "total_loss = 0\n",
        "total = 0\n",
        "for iter, testdata in enumerate(test_loader):\n",
        "    test_inputs, test_labels = testdata\n",
        "    test_labels = torch.squeeze(test_labels)\n",
        "    if test_labels.shape[0] !=64:\n",
        "      continue\n",
        "    test_inputs = test_inputs.to(device)\n",
        "    test_labels = test_labels.to(device)\n",
        "\n",
        "    output = LSTM_model(test_inputs.t())\n",
        "    loss = loss_function(output, Variable(test_labels))\n",
        "    _, predicted = torch.max(output.data, 1)\n",
        "    total_acc = total_acc + (predicted == test_labels).sum()\n",
        "    total_loss = total_loss+loss.item()\n",
        "    total = total + len(test_labels)\n",
        "print(f\"Total Loss {total_loss / total}. Total Acc {total_acc / total}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iz6GUkQvpStb",
        "outputId": "83bbd6cd-be1c-484b-d661-d2dda87691d3"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Loss 0.01070122863141654. Total Acc 0.5690281391143799\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT Model"
      ],
      "metadata": {
        "id": "uANfo14KXJgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare for the data\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "input_ids = []     \n",
        "input_types = []   \n",
        "input_masks = []   \n",
        "label = []         \n",
        "pad_size = 32      \n",
        " \n",
        "for i in range(data.shape[0]): \n",
        "    x = data.iloc[i,1]\n",
        "    y = data.iloc[i,7]\n",
        "    x = tokenizer.tokenize(x)\n",
        "    x = x[:510]\n",
        "    tokens = [\"[CLS]\"] + x + [\"[SEP]\"]\n",
        "    ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    types = [0] *(len(ids))\n",
        "    masks = [1] * len(ids)\n",
        "    if len(ids) < pad_size:\n",
        "        types = types + [1] * (pad_size - len(ids))\n",
        "        masks = masks + [0] * (pad_size - len(ids))\n",
        "        ids = ids + [0] * (pad_size - len(ids))\n",
        "    else:\n",
        "        types = types[:pad_size]\n",
        "        masks = masks[:pad_size]\n",
        "        ids = ids[:pad_size]\n",
        "    input_ids.append(ids)\n",
        "    input_types.append(types)\n",
        "    input_masks.append(masks)\n",
        "    label.append([int(y)])\n"
      ],
      "metadata": {
        "id": "ehNiGkH0pJl5"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_index = list(range(len(input_ids)))\n",
        "np.random.seed(2022)  \n",
        "np.random.shuffle(split_index)\n",
        " \n",
        "input_ids_train = np.array([input_ids[i] for i in split_index[:int(len(input_ids)*0.8)]])\n",
        "input_types_train = np.array([input_types[i] for i in split_index[:int(len(input_ids)*0.8)]])\n",
        "input_masks_train = np.array([input_masks[i] for i in split_index[:int(len(input_ids)*0.8)]])\n",
        "y_train = np.array([label[i] for i in split_index[:int(len(input_ids) * 0.8)]])\n",
        " \n",
        "input_ids_test = np.array([input_ids[i] for i in split_index[int(len(input_ids)*0.8):]])\n",
        "input_types_test = np.array([input_types[i] for i in split_index[int(len(input_ids)*0.8):]])\n",
        "input_masks_test = np.array([input_masks[i] for i in split_index[int(len(input_ids)*0.8):]])\n",
        "y_test = np.array([label[i] for i in split_index[int(len(input_ids) * 0.8):]])\n"
      ],
      "metadata": {
        "id": "t8ZQJVYEpLLK"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids_train = input_ids_train[:489408]\n",
        "input_types_train = input_types_train[:489408]\n",
        "input_masks_train = input_masks_train[:489408]\n",
        "y_train = y_train[:489408]\n",
        "\n",
        "input_ids_test = input_ids_test[:122304]\n",
        "input_types_test = input_types_test[:122304]\n",
        "input_masks_test = input_masks_test[:122304]\n",
        "y_test = y_test[:122304]"
      ],
      "metadata": {
        "id": "s7riipzgpM6W"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "train_data = TensorDataset(torch.LongTensor(input_ids_train), \n",
        "                           torch.LongTensor(input_types_train), \n",
        "                           torch.LongTensor(input_masks_train), \n",
        "                           torch.LongTensor(y_train))\n",
        "train_sampler = RandomSampler(train_data)  \n",
        "train_loader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        " \n",
        "test_data = TensorDataset(torch.LongTensor(input_ids_test), \n",
        "                          torch.LongTensor(input_types_test), \n",
        "                         torch.LongTensor(input_masks_test),\n",
        "                          torch.LongTensor(y_test))\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_loader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n"
      ],
      "metadata": {
        "id": "bLGC1VjppN-t"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model part\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\") \n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = True \n",
        "        self.fc = nn.Linear(768, 2)\n",
        " \n",
        "    def forward(self, x):\n",
        "        context = x[0]\n",
        "        types = x[1]\n",
        "        mask = x[2]\n",
        "        _, output = self.bert(context, token_type_ids=types, \n",
        "                              attention_mask=mask, \n",
        "                              output_all_encoded_layers=False)\n",
        "        result = self.fc(output)\n",
        "        return result\n"
      ],
      "metadata": {
        "id": "_SWDJsr8pPLj"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Model().to(DEVICE)\n",
        "print(model) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Og23HIdpQwk",
        "outputId": "1b8202ab-8acd-42fc-9c24-d1d2341f9659"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(\n",
            "  (bert): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (token_type_embeddings): Embedding(2, 768)\n",
            "      (LayerNorm): BertLayerNorm()\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): BertLayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (fc): Linear(in_features=768, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5) "
      ],
      "metadata": {
        "id": "OKBbB9oIpSEQ"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (x1,x2,x3, y) in enumerate(train_loader):\n",
        "        x1,x2,x3, y = x1.to(device), x2.to(device), x3.to(device), y.to(device)\n",
        "        y_pred = model([x1, x2, x3]) \n",
        "        model.zero_grad()       \n",
        "        loss = F.cross_entropy(y_pred, y.squeeze()) \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if(batch_idx + 1) % 100 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.2f}%)] Loss: {:.6f}'.format(epoch, (batch_idx+1) * len(x1), \n",
        "                                                                           len(train_loader.dataset),\n",
        "                                                                           100. * batch_idx / len(train_loader), \n",
        "                                                                           loss.item())) \n",
        " \n",
        "def test(model, device, test_loader): \n",
        "    model.eval()\n",
        "    test_loss = 0 \n",
        "    acc = 0\n",
        "    for batch_idx, (x1,x2,x3, y) in enumerate(test_loader):\n",
        "        x1,x2,x3, y = x1.to(device), x2.to(device), x3.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "            y_ = model([x1,x2,x3])\n",
        "        test_loss += F.cross_entropy(y_, y.squeeze())\n",
        "        pred = y_.max(-1, keepdim=True)[1]  \n",
        "        acc += pred.eq(y.view_as(pred)).sum().item()   \n",
        "    test_loss /= len(test_loader)\n",
        "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
        "          test_loss, acc, len(test_loader.dataset),\n",
        "          100. * acc / len(test_loader.dataset)))\n"
      ],
      "metadata": {
        "id": "PsSZ6HxfpUtv"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train and test then same the model - need to run again with a different dataset\n",
        "\n",
        "PATH = 'bert_model.pth'\n",
        "epoch = 1\n",
        "train(model, DEVICE, train_loader, optimizer, epoch)\n",
        "test(model, DEVICE, test_loader)\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_3oxo3YpV9c",
        "outputId": "6256ace5-733f-41ff-89da-95ba330c8b55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [6400/489408 (1.29%)] Loss: 0.701379\n",
            "Train Epoch: 1 [12800/489408 (2.60%)] Loss: 0.708586\n",
            "Train Epoch: 1 [19200/489408 (3.91%)] Loss: 0.684898\n",
            "Train Epoch: 1 [25600/489408 (5.22%)] Loss: 0.661870\n",
            "Train Epoch: 1 [32000/489408 (6.53%)] Loss: 0.698192\n",
            "Train Epoch: 1 [38400/489408 (7.83%)] Loss: 0.696203\n",
            "Train Epoch: 1 [44800/489408 (9.14%)] Loss: 0.640521\n",
            "Train Epoch: 1 [51200/489408 (10.45%)] Loss: 0.652103\n",
            "Train Epoch: 1 [57600/489408 (11.76%)] Loss: 0.678355\n",
            "Train Epoch: 1 [64000/489408 (13.06%)] Loss: 0.674060\n",
            "Train Epoch: 1 [70400/489408 (14.37%)] Loss: 0.706404\n",
            "Train Epoch: 1 [76800/489408 (15.68%)] Loss: 0.673341\n",
            "Train Epoch: 1 [83200/489408 (16.99%)] Loss: 0.680493\n",
            "Train Epoch: 1 [89600/489408 (18.29%)] Loss: 0.686105\n",
            "Train Epoch: 1 [96000/489408 (19.60%)] Loss: 0.719447\n",
            "Train Epoch: 1 [102400/489408 (20.91%)] Loss: 0.652460\n",
            "Train Epoch: 1 [108800/489408 (22.22%)] Loss: 0.723900\n",
            "Train Epoch: 1 [115200/489408 (23.53%)] Loss: 0.700157\n",
            "Train Epoch: 1 [121600/489408 (24.83%)] Loss: 0.644390\n",
            "Train Epoch: 1 [128000/489408 (26.14%)] Loss: 0.657213\n",
            "Train Epoch: 1 [134400/489408 (27.45%)] Loss: 0.699539\n",
            "Train Epoch: 1 [140800/489408 (28.76%)] Loss: 0.680894\n",
            "Train Epoch: 1 [147200/489408 (30.06%)] Loss: 0.676008\n",
            "Train Epoch: 1 [153600/489408 (31.37%)] Loss: 0.667583\n",
            "Train Epoch: 1 [160000/489408 (32.68%)] Loss: 0.704850\n",
            "Train Epoch: 1 [166400/489408 (33.99%)] Loss: 0.678000\n",
            "Train Epoch: 1 [172800/489408 (35.29%)] Loss: 0.657336\n",
            "Train Epoch: 1 [179200/489408 (36.60%)] Loss: 0.683566\n",
            "Train Epoch: 1 [185600/489408 (37.91%)] Loss: 0.695828\n",
            "Train Epoch: 1 [192000/489408 (39.22%)] Loss: 0.670021\n",
            "Train Epoch: 1 [198400/489408 (40.53%)] Loss: 0.672814\n",
            "Train Epoch: 1 [204800/489408 (41.83%)] Loss: 0.637776\n",
            "Train Epoch: 1 [211200/489408 (43.14%)] Loss: 0.679509\n",
            "Train Epoch: 1 [217600/489408 (44.45%)] Loss: 0.695315\n",
            "Train Epoch: 1 [224000/489408 (45.76%)] Loss: 0.649024\n",
            "Train Epoch: 1 [230400/489408 (47.06%)] Loss: 0.673482\n",
            "Train Epoch: 1 [236800/489408 (48.37%)] Loss: 0.725140\n",
            "Train Epoch: 1 [243200/489408 (49.68%)] Loss: 0.652200\n",
            "Train Epoch: 1 [249600/489408 (50.99%)] Loss: 0.698266\n",
            "Train Epoch: 1 [256000/489408 (52.30%)] Loss: 0.633608\n",
            "Train Epoch: 1 [262400/489408 (53.60%)] Loss: 0.642149\n",
            "Train Epoch: 1 [268800/489408 (54.91%)] Loss: 0.668130\n",
            "Train Epoch: 1 [275200/489408 (56.22%)] Loss: 0.622409\n",
            "Train Epoch: 1 [281600/489408 (57.53%)] Loss: 0.670970\n",
            "Train Epoch: 1 [288000/489408 (58.83%)] Loss: 0.690629\n",
            "Train Epoch: 1 [294400/489408 (60.14%)] Loss: 0.695169\n",
            "Train Epoch: 1 [300800/489408 (61.45%)] Loss: 0.646889\n",
            "Train Epoch: 1 [307200/489408 (62.76%)] Loss: 0.722290\n",
            "Train Epoch: 1 [313600/489408 (64.06%)] Loss: 0.667565\n",
            "Train Epoch: 1 [320000/489408 (65.37%)] Loss: 0.642469\n",
            "Train Epoch: 1 [326400/489408 (66.68%)] Loss: 0.636908\n",
            "Train Epoch: 1 [332800/489408 (67.99%)] Loss: 0.698560\n",
            "Train Epoch: 1 [339200/489408 (69.30%)] Loss: 0.692234\n",
            "Train Epoch: 1 [345600/489408 (70.60%)] Loss: 0.655969\n",
            "Train Epoch: 1 [352000/489408 (71.91%)] Loss: 0.644575\n",
            "Train Epoch: 1 [358400/489408 (73.22%)] Loss: 0.649271\n",
            "Train Epoch: 1 [364800/489408 (74.53%)] Loss: 0.688643\n",
            "Train Epoch: 1 [371200/489408 (75.83%)] Loss: 0.695600\n",
            "Train Epoch: 1 [377600/489408 (77.14%)] Loss: 0.675556\n",
            "Train Epoch: 1 [384000/489408 (78.45%)] Loss: 0.675600\n",
            "Train Epoch: 1 [390400/489408 (79.76%)] Loss: 0.628439\n",
            "Train Epoch: 1 [396800/489408 (81.06%)] Loss: 0.688652\n",
            "Train Epoch: 1 [403200/489408 (82.37%)] Loss: 0.661353\n",
            "Train Epoch: 1 [409600/489408 (83.68%)] Loss: 0.695582\n",
            "Train Epoch: 1 [416000/489408 (84.99%)] Loss: 0.659739\n",
            "Train Epoch: 1 [422400/489408 (86.30%)] Loss: 0.672622\n",
            "Train Epoch: 1 [428800/489408 (87.60%)] Loss: 0.660091\n",
            "Train Epoch: 1 [435200/489408 (88.91%)] Loss: 0.644243\n",
            "Train Epoch: 1 [441600/489408 (90.22%)] Loss: 0.634221\n",
            "Train Epoch: 1 [448000/489408 (91.53%)] Loss: 0.686925\n",
            "Train Epoch: 1 [454400/489408 (92.83%)] Loss: 0.672303\n",
            "Train Epoch: 1 [460800/489408 (94.14%)] Loss: 0.670790\n",
            "Train Epoch: 1 [467200/489408 (95.45%)] Loss: 0.665138\n",
            "Train Epoch: 1 [473600/489408 (96.76%)] Loss: 0.656146\n",
            "Train Epoch: 1 [480000/489408 (98.06%)] Loss: 0.700197\n",
            "Train Epoch: 1 [486400/489408 (99.37%)] Loss: 0.659111\n",
            "Test set: Average loss: 0.6671, Accuracy: 72524/122304 (59.30%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load model - run with current dataset\n",
        "\n",
        "model = Model().to(DEVICE)\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/CS410Project/bert_model.pth\"))\n",
        "test(model, DEVICE, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovgwELJGqein",
        "outputId": "9e646194-3245-4d79-b12e-e87ee5677e96"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Average loss: 0.6671, Accuracy: 72524/122304 (59.30%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k3tHgVhqGDjx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}